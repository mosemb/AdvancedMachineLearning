{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso & Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse models for variable selection\n",
    "\n",
    "The goal of this lab is to familiarize with the concept of *sparsity* and to see its connection with the selection of variables in high dimensional contexts.\n",
    "\n",
    "A machine learning model is said to be **sparse** when it only contains a small number of non-zero parameters, with respect to the number of features that can be measured on the objects this model represents.\n",
    "\n",
    "Let's see how sparsity works in practice.\n",
    "\n",
    "## 1. *Sparse* variable selection on toy problems\n",
    "\n",
    "Using `scikit-learn` let's make a toy regression problem with `n=100` samples and `d=30` variables of which only `d_rel=5` are informative. Use the flag `coef=True` to get $w^*$, *ie* the **real** weights of the toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "d = 30\n",
    "d_rel = 7\n",
    "\n",
    "X, y, coef = make_regression(n_samples=n, n_features=d, n_informative=d_rel, noise=20, coef=True, random_state=0)\n",
    "print(\"Data shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform sparse linear regression fitting a [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model\n",
    "\n",
    "$$\\min_w \\frac{1}{2 n} ||y - Xw||^2_2 + \\alpha~||w||_1$$\n",
    "\n",
    "for a fixed value of $\\alpha=40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Lasso(alpha=40)\n",
    "mdl.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the value of the weights $w$ by checking `mdl.coef_`, let's plot their value on a 2D space and let's compare them with $w^*$. \n",
    "\n",
    "**Comment the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to play a bit with the value of $\\alpha$.\n",
    "\n",
    "**What do you expect?**\n",
    "\n",
    "**What happens?**\n",
    "\n",
    "**What can you say about the choice of setting $\\alpha=40$ in the previous example?**\n",
    "\n",
    "## 2. Recap on the regularization path\n",
    "\n",
    "We sensed that the role played by the regularization parameter $\\alpha$ is crucial. When performing variable selection in high-dimensional contexts it may be helpful to observe the weight assigned to each variable for increasing values of $\\alpha$.\n",
    "\n",
    "An intuitive representation of such phenomenon is called **`regularization path`**\n",
    "\n",
    "### 2.1 Hands on the Lasso path\n",
    "\n",
    "**With the toy regression problem above, implement a function that estimates the `lasso path` and visualize it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement a function that estimates the `ridge path`.Compare your results and comment differences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lasso and correlations among features\n",
    "\n",
    "In presence of correlation between variables, the lasso penalty may not be very informative. Let's try to evaluate it on a toy dataset in which each informative variable is repeated twice.\n",
    "\n",
    "Use `n=100` samples, `half_d_rel=5`, `d_dummy = 25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "half_d_rel = 5\n",
    "d = 30\n",
    "d_dummy = d - half_d_rel\n",
    "\n",
    "X, y, coef = make_regression(n_samples=n, n_features=d-half_d_rel, n_informative=half_d_rel, coef=True)\n",
    "relevant = np.nonzero(coef)[0]\n",
    "X = np.hstack((X, -2*X[:,relevant]))\n",
    "\n",
    "print(\"Data shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The total number of relevant variables here is `2*half_d_rel = 10`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the function implemented in Exercise #2.1 and visualize the lasso path for this dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many selected variable do you see? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Elastic-Net` model :\n",
    "\n",
    "$$\\min_w \\frac{1}{2n} ||y - Xw||^2_2 + \\alpha \\cdot l_{1_{ratio}} \\cdot ||w||_1 + \\frac{1}{2}~\\alpha \\cdot (1 - l_{1_{ratio}}) \\cdot ||w||^2_2$$\n",
    "\n",
    "or equivalently as:\n",
    "\n",
    "$$\\min_w \\frac{1}{2n} ||y - Xw||^2_2 + \\tau~||w||_1 + \\mu~||w||^2_2$$\n",
    "\n",
    "with appropriate parameters formulation.\n",
    "The Elastic-Net, thanks to the combined influence of the $\\ell_1$ and the $\\ell_2$-norm, achieves a sparse and stable solution in which joint selection of collinear variables is promoted.\n",
    "\n",
    "**Fix the `l_1ratio=0.5` and evaluate the Elastic-Net path on the datasets of Exercise 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many selected variable do you see? Why?**\n",
    "\n",
    "**Which is the asymptotic behavior of the weights corresponding to correlated features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Elastic-net for variable selection in Microarray study\n",
    "\n",
    "**We will train an ElasticNetClassifier (take a look at the file `enet_classifier.py`) on the [Golub dataset](http://portals.broadinstitute.org/cgi-bin/cancer/publications/pub_paper.cgi?paper_id=43) which contains microarray data measured from two classes of leukemia: acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from enet_classifier import ElasticNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load data\n",
    "\n",
    "Let's load the dataset with `pandas` in the usual way (`pandas.read_csv`). Read both data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gedm.csv\", header=0, index_col=0).T\n",
    "print(\"n_samples = {} | n_variables = {}\".format(*data.shape))\n",
    "\n",
    "n_samples, n_variables = data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv', header=0, index_col=0)\n",
    "print(\"n_samples (AML) = {} | n_samples (ALL) = {}\".format(\n",
    "    np.sum(labels.values == 'AML'), np.sum(labels.values == 'ALL')))\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode the labels into the standard binary classification problem: associate ALL with class `1` and AML with class `-1`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = np.where(labels.values == 'ALL', 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the accuracy score of a random classifier?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that some columns of `data` start with `AFFX`. These are not real features. Instead, they are some control probes related to the microarray structure. For this reason, we can remove them before the actual analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = data.columns[~data.columns.str.startswith(\"AFFX\")]\n",
    "data = data.loc[:, relevant_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data visualization\n",
    "\n",
    "**We can visualize the data by projecting them on a 2-dimensional space with `PCA`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fitting the model\n",
    "\n",
    "To control separately $\\mu$ and $\\tau$ in the ElasticNet model, you need to solve the appropriate linear system of equationa.\n",
    "\n",
    "$ \\alpha l_{1_{ratio}} = \\tau$\n",
    "\n",
    "$ \\frac{\\alpha}{2} (1 - l_{1_{ratio}}) = \\mu$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "$ l_{1_{ratio}} = \\frac{\\tau}{2 \\mu + \\tau}$\n",
    "\n",
    "$ \\alpha = 2 \\mu + \\tau$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a Python function which performs the conversion from `tau` and `mu` to `alpha` and `l1_ratio`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Nested variable selection\n",
    "\n",
    "We will observe the effect of increasing the value of `mu`, which controls the amount of variable correlation tolerated in the solution. For example, with fixed `tau`, increasing `mu` should result in selecting nested list of variables, as in [DeMol09](http://online.liebertpub.com/doi/abs/10.1089/cmb.2008.0171).\n",
    "\n",
    "**Implement a data analysis pipeline following the next steps:**\n",
    "\n",
    "1. Fix a value for `mu_0`, *e.g.* `mu_0 = 1`.\n",
    "\n",
    "2. Split the dataset into `K` folds (non-overlapping groups).\n",
    "\n",
    "    a. For each iteration keep $\\frac{1}{K}$ samples aside and use it as test set.\n",
    "    \n",
    "    b. Use the remaining $\\frac{K-1}{K}$ samples and use them as training set to optimize the regularization parameter `tau` via an inner `GridSearch` cross-validation.\n",
    "    \n",
    "    c. The best model is achieved with the optimal `tau` fitted on the training set.\n",
    "\n",
    "    d. Evaluate the best model on the test set and keep track of the accuracy score and the list of selected variables.\n",
    "    \n",
    "This would allow us to obtain a ranking of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = #training data, labels\n",
    "\n",
    "#Find an appropriate number of folds in which split the dataset\n",
    "K = #?\n",
    "\n",
    "kf = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "\n",
    "enet = ElasticNetClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following ranges for the parameters:\n",
    "- `tau_range` in logarithmic scale from `1e-1` to `1e5`\n",
    "- `mu_range` as [`1e4`, `1e5`, `1e7`, `1e8`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Build a `pandas` DataFrame whose values correspond to the number of times each variable is selected for different values of $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Build an explanatory visualization of the selection frequency for each variable, at different choices of $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
