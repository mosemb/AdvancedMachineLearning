{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h-pNS_KaIbZ"
   },
   "source": [
    "# Lab 3: Deep Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUhXzbFAE3BM"
   },
   "source": [
    "The third lab session is about data that have a sequential structure that must be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4TNAZGt4aIbe"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os, json, re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnmA6CbEaIbv"
   },
   "source": [
    "# 3.1 Deal with sequential data\n",
    "<img src=\"https://drive.google.com/uc?id=1k6HwtWlMTkVJFuyBpeBGF2sApqxR-KRc\" width=\"600px\" align=\"right\"><br>\n",
    "In this lab we see Deep Learning models that can process sequential data (text, timeseries,..).<br>\n",
    "These models don’t take as input raw text: they only work with numeric tensors; **vectorizing** text is the process of transforming text into numeric tensors.<br><br><br>\n",
    "The different units into which you can break down text (words, characters) are called tokens; then if you apply a tokenization scheme, you associate numeric vectors with the generated tokens.<br>\n",
    "These vectors, packed into sequence tensors, are fed into Deep Neural Network.<br>\n",
    "There are multiple ways to associate a vector with a token: we will see One-Hot Encoding and Token Embedding.<br>\n",
    "In this section we are going to deal with:\n",
    "* 3.1.1 One-Hot Encoding\n",
    "* 3.1.2 Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhRTCGvGaIbz"
   },
   "source": [
    "## 3.1.1 One-Hot Encoding\n",
    "One-Hot Encoding consists of associating a unique integer index with every word and then turning this integer index $i$ into a binary vector of size $N$ (the size of the vocabulary); the vector is all zeros except for the $i$-th entry, which is 1.\n",
    "<img src=\"https://drive.google.com/uc?id=1OzK9t_WXQsaDuZoOTQSksLuNMubXm0gc\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8hfjPNJaIb1"
   },
   "source": [
    "#### Try to perform an Encoding using Tokenizer\n",
    "Keras provides the Tokenizer class for preparing text documents for DL.<br>\n",
    "The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EPyke25aIb3",
    "outputId": "4d4aabb3-9b5d-4d8d-e7af-24790731ab6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# define 4 documents\n",
    "docs = ['Well done!','Good work','Great effort','nice work']\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDycrywyaIb-"
   },
   "source": [
    "Some problems related to this kind of encoding are sparsity of the solution and the high dimensionality of the vector encoding of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHeXmpPfaIcB"
   },
   "source": [
    "## 3.1.2 Word embedding\n",
    "<img src=\"https://drive.google.com/uc?id=1YRcQ1_5n8Qay0GFoSLVrNcEKWeik5G3S\" width=\"400px\" align=\"right\"><br>\n",
    "The vector obtained from word embedding is dense and has lower dimensionality w.r.t One-Hot Encoding vector; the dimensionality of embedding space vector is an hyperparameter.<br>\n",
    "There are two ways to obtain word embeddings:<br>\n",
    "* May be learned jointly with the network\n",
    "* May use pre-trained word vectors (Word2Vec, GloVe,..)\n",
    "\n",
    "\n",
    "Word embeddings maps human language into a geometric space; in a reasonable embedding space synonyms are embedded into similar word vectors and the geometric distance between any two word vectors reflects the semantic distance between the associated words (words meaning different things are embedded at points far away from each other, whereas related words are closer).<br>\n",
    "How good is a word-embedding space depends on the specific task.<br>\n",
    "It is reasonable to learn a new embedding space with every new task: with backpropagation and Keras it reduces to learn the weights of the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFHFL8wSaIcF"
   },
   "source": [
    "### Learning Word Embeddings with the embedding layer\n",
    "#### Load imdb dataset\n",
    "This dataset contains movies reviews from IMDB, labeled by sentiment(positive/negative); reviews have been preprocessed, and each review is encoded as a sequence of word indexes(integers).<br>\n",
    "https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvElh7GME3BQ",
    "outputId": "8522da48-bcc3-46d0-ccc1-f55f50342893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "maxlen = 50\n",
    "\n",
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5muY3QYaIcK"
   },
   "source": [
    "#### Show the size of vocabulary and the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXB3wghhaIcL"
   },
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print('Vocab size : ', vocab_size)\n",
    "\n",
    "\n",
    "words_freq_list =words_freq_list = []\n",
    "[]\n",
    "for (k,v) in imdb.get_word_index().items():\n",
    "    words_freq_list.append((k,v))\n",
    "\n",
    "sorted_list = sorted(words_freq_list, key=lambda x: x[1])\n",
    "\n",
    "print(\"50 most common words: \\n\")\n",
    "print(sorted_list[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27uI4g4saIcQ"
   },
   "outputs": [],
   "source": [
    "word_to_index['otherwise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXjwll2WaIda"
   },
   "source": [
    "# 3.2 Recurrent Neural Network\n",
    "Here https://colah.github.io/posts/2015-08-Understanding-LSTMs/ you can find a clear explanation about RNNs and LSTMs; the following is a summary of the main concepts.\n",
    "\n",
    "\n",
    "A major characteristic of some neural networks, as ConvNet, is that they have no memory: each input is processed independently, with no state kept in between inputs. Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in.<br>\n",
    "A recurrent neural network (RNN) adopts the same principle but in an extremely simplified version: it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1RaDXXygc0HJv6YyIAjU4_Nbw1bXzAhAJ\" width=\"650px\"><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each input $x_{i=t-1, t, t+1, ..}$ is combined with the internal state and then is applied an activation function (e.g. $tanh$); then the output is computed $h_{i=t-1, t, t+1, ..}$ and the internal state is updated.<br>\n",
    "In many cases, you just need the last output ($h_{i=last t}$ at the end of the loop), because it already contains information\n",
    "about the entire sequence.\n",
    "<img src=\"https://drive.google.com/uc?id=1RtulDLKQnzZTSbBsD2n7TIlRVEaESB8o\" width=\"550px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL10YVWTaIdi"
   },
   "source": [
    "#### Create the model\n",
    "In the following sections we will develop different models. Be careful to the fact that we are dealing with a binary classification problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PCby93EaIdi"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features, 16))\n",
    "\n",
    "# Complete the model, it should be made by at least:\n",
    "# 1 SimpleRNN layer\n",
    "# 1 Dense layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JTBSi5TaIdp"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DamvGVOpaIdq"
   },
   "outputs": [],
   "source": [
    "# Train your model here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5vtqdoL8gQl"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt-M_A7OaIds"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pZ5tcAjaIdx"
   },
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDbsgw_5aIdy"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is6FCRVRaId0"
   },
   "source": [
    "#### Try to build a new model where you stack several recurrent layers.\n",
    "In such a setup, you have to get all of the intermediate layers to return full sequence of outputs. This is needed to return batch size, timesteps, hidden state. By doing this the output should contain all historical generated outputs along with time stamps (3D). This way the next layer can work further on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eExHNhMraId1"
   },
   "outputs": [],
   "source": [
    "# Build the model. It should be made by at least:\n",
    "# 1 Embedding layer\n",
    "# More than 1 SimpleRNN layer, do not forget to put the return_sequences parameter to True \n",
    "# 1 Dense layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJlV5HynaId_"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWm__m8RaIeC"
   },
   "outputs": [],
   "source": [
    "# Train your model here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNdh2-0haIeI"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEdVU-XYaIeL"
   },
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNerI-XraIeM"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8rKKlLhE3BU"
   },
   "source": [
    "**What can you say about the obtained results? What about the comparison between these results and the ones obtained in the single layer RNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdJA3ttvaIeP"
   },
   "source": [
    "# 3.3 LSTM Network\n",
    "LSTMs are a special kind of recurrent neural network which works, for many tasks, much better than the standard RNNs.<br>\n",
    "These nets are capable of learning long-term dependencies (they are explicitly designed to avoid the long-term dependency problem); remembering information for long periods of time is practically their default behavior.<br><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1CQ0RkQrnIkr1dFqSU1qHfPh5sJe_A6pQ\" width=\"650px\"><br>\n",
    "\n",
    "RNNs have a very simple structure, such as a single $tanh$ layer.<br>\n",
    "LSTMs also have a chain like structure, but the repeating module has a different structure: instead of having a single neural network layer, there are four, interacting in a very special way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0KJgSymaIeR"
   },
   "source": [
    "#### Create LSTM model in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d15KAMKpaIeR"
   },
   "outputs": [],
   "source": [
    "# Build the model. It should be made by at least:\n",
    "# 1 Embedding layer\n",
    "# 1 LSTM layer\n",
    "# 1 Dense layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPz5KvjAaIeV"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6QTdfzZaIeW"
   },
   "outputs": [],
   "source": [
    "# Train your model here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XELeM5_KaIef"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPSLRTciaIei"
   },
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovW6sEhNaIej"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9nY8rzCE3BV"
   },
   "source": [
    "# 3.4 Reuters newswire classification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hti_vR3uE3BW"
   },
   "source": [
    "The reuters newswire classification dataset is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. More information about the dataset and how to use it can be found here:\n",
    "https://keras.io/api/datasets/reuters/\n",
    "\n",
    "Try to build a new model dealing with this new dataset.\n",
    "Try to use both the RNN and the LSTM approach, and select the best of them. What do you expect will be the best? Be carefull that this domain shift will imply some changes in your code as it is not a binary classification problem anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOkh83WjE3BW"
   },
   "outputs": [],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=max_features)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJQOjCdNE3BW"
   },
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "415cLXTkE3BW"
   },
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nTYHdjRE3BW"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQXlm9TqE3BW"
   },
   "outputs": [],
   "source": [
    "# Train your model here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKrAL4XpE3BW"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7iBB05xE3BX"
   },
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1guoI8XE3BX"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFpkRx2mE3BX"
   },
   "source": [
    "**Are the results accordant to what you expected? Can you notice some differences between the RNN and the LSTM results? Why?**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TempNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
