{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-thumb",
   "metadata": {},
   "source": [
    "# Model selection and  assessment\n",
    "\n",
    "We discussed during class why it is appropriate to hold-out part of the data as a test dataset\n",
    "\n",
    "$$ \\hat{L} = \\frac{1}{n_T} \\sum_{(x,y)\\in T} \\ell(y, \\hat{f}(x)), \\quad \\textit{TRAINING ERROR} $$\n",
    "\n",
    "$$ \\hat{L}_{HO} = \\frac{1}{n_{HO}} \\sum_{(x,y)\\in HO} \\ell(y, \\hat{f}(x)), \\quad \\textit{HOLD-OUT ERROR} $$\n",
    "\n",
    "with $\\hat{f}$ being the solution of the training set $T$.\n",
    "\n",
    "In actual applications, there are often two steps to solving a prediction problem: model selection and model assessment. In model selection we estimate the performance of various competing models with the hope of choosing the best one. Having chosen the final model, we assess the model by estimating the prediction error on new unseen data.\n",
    "\n",
    "Class of models $\\hat{f}_{\\alpha}$: one should not use the test dataset for selecting $\\alpha$. \n",
    "\n",
    "One could divide the data in three parts: \n",
    "- train;\n",
    "- validation;\n",
    "- (hold-out) test.\n",
    "\n",
    "We use the training and validation data to select the best model and the test data to assess the chosen model.\n",
    "The recipe is the following:\n",
    "\n",
    "1.  We train all competing model on the train data and define the best model as the one that predicts best in the validation set.  We could re-split the train/validation data, do this many times, and select the method that, on average, performs the best.\n",
    "\n",
    "2.  We chose the best model among many competitors, hence the observed performance will be a bit biased. Therefore, to appropriately assess performance on independent data we look at the score on the test set.\n",
    "\n",
    "\n",
    "A common problem: when the amout of data is limited, the results from fitting a model to splitted data can be substantially different to fitting to the complete dataset. Indeed, when the number of samples is small (smaller than the number of variables) we do not have enough examples to approach the problem as described above. Thus, we need to perform a cross-validation procedure that returns a mean error score on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-privilege",
   "metadata": {},
   "source": [
    "## Exercise #1: Splitting Techniques\n",
    "\n",
    "Given the California-housing dataset, use the following splitting methods:\n",
    "- **K-fold** : split dataset into k consecutive folds. Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "- **Monte Carlo** : randomly split the dataset into training and test sets(we need to specify the percentage of the test data points and an initial random state for reshuffling the data. We set `test_size = 0.2` and `random_state = 1`).\n",
    "- **Leave One Out** : provides train/test indices to split data in train/test sets. Each sample is used once as a test set (singleton) while the remaining samples form the training set.\n",
    "\n",
    "**Compute the mean and standard deviation at different splits for each feature and plot them. Use 5 splits for K-fold and MC. Comment on the results**\n",
    "\n",
    "When performing data-splitting, you can refer to the classes in [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "closed-webster",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f25eccd1edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-knowing",
   "metadata": {},
   "source": [
    "### Imputation Using (Mean/Median) Values:\n",
    "Many real-world datasets may contain missing values for various reasons.\n",
    "Training a model with a dataset that has a lot of missing values can drastically impact the model’s quality.\n",
    "One way to handle this problem is to get rid of the observations that have missing data. However, you will risk losing data points with valuable information. A better strategy would be to impute the missing values.\n",
    "\n",
    "\n",
    "One strategy of imputing is to compute the mean/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others.\n",
    "\n",
    "**Among the previous strategies of splitting, which one would you suggest to use for imputing missing data? Motivate your answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-stereo",
   "metadata": {},
   "source": [
    "## Excercise #2: Cross-validation\n",
    "\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test`, `y_test`.\n",
    "\n",
    "When evaluating different settings for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a validation set: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called **cross-validation**. A test set should still be held out for final evaluation, but a fixed validation set is no longer needed when doing CV and the model sees all the data (minus the test set). Cross-validation can be iteratively performed adopting different splitting techniques. For example, in `K-Fold CV`, the training set is split into K smaller sets. This procedure is followed for each of the K “folds”:\n",
    "- A model is trained using K-1 folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (using some performance measure, such as accuracy).\n",
    "\n",
    "The performance measure reported by K-Fold cross-validation is then the average of the values computed in the loop.\n",
    "\n",
    "Below a schamatic representation of 5-Fold cross-validation.\n",
    "\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"550\" height=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "**Generate a labelled dataset for binary calssification using [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) with 600 samples, 2 classes and 200 features, use as classificator [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier) and look for the regularization parameters in the interval $[10^{-5}, 10^{15}]$ using different splitting schemes. Hold out a test set and compare performances of different models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=600, n_features=200, flip_y=0.05, class_sep=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-district",
   "metadata": {},
   "source": [
    "**Repeat the previous procedure using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and compare the selected parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-portal",
   "metadata": {},
   "source": [
    "## Exercise #3: Unbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-fight",
   "metadata": {},
   "source": [
    "When dealing with unbalanced classes, caution must be used when splitting data into train/validation/test.\n",
    "\n",
    "**Can you guess why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-puzzle",
   "metadata": {},
   "source": [
    "**Use both Ridge Classifier and Logistic Regression for classification on the dataset** [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud). \n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders.This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. We further select the number of non-fraudulent transations so that the imbalance is less severe and the ratio is 5%.\n",
    "\n",
    "**Make use of KFold and StratifiedKFold techniques for splitting data and two different scores during training/validation : `accuracy` and `balanced accuracy`.**\n",
    "\n",
    "The `accuracy` in binary classification is defined as \n",
    "\n",
    "$$\\dfrac{TN+TP}{TN+TP+FP+FN}$$ \n",
    "\n",
    "it quantifies the proportion of true results among the total number of cases examined. While the `balanced accuracy` is defined as:\n",
    "\n",
    "$$\\frac{1}{2}\\,\\bigg{[}\\dfrac{TP}{TP+FN}+\\dfrac{TN}{TN+FP}\\bigg{]}$$\n",
    "\n",
    "which is the average of recall obtained on each class.\n",
    "\n",
    "**What can you conclude by the comparison of the trained models performances? Motivate your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('creditcard.csv')\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit.drop(['Time','Class'],axis = 1).values\n",
    "y = credit['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "pos_ind = np.where(y==1)[0]\n",
    "neg_ind = np.random.choice(np.where(y==0)[0],9840,replace = False)\n",
    "ind = np.sort(np.hstack((pos_ind,neg_ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[ind,:]\n",
    "y = y[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-municipality",
   "metadata": {},
   "source": [
    "**Extra: can you think about other strategies to deal with unbalanced datasets?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
